{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdtEYp4s1XNY"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Colab Fast Start (Around 5 minutes)\n",
        "# Clone the repositories\n",
        "!git clone https://github.com/NVlabs/stylegan3\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/salesforce/BLIP\n",
        "\n",
        "# Install the requirements\n",
        "!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 ninja==1.11.1.1\n",
        "!pip install ftfy==6.1.3 regex==2023.12.25 tqdm==4.66.2\n",
        "!pip install transformers==4.19.4 timm==0.9.16 fairscale==0.4.13\n",
        "\n",
        "# Download the pre-trained models\n",
        "!mkdir pretrained_models\n",
        "!gdown -O test.png https://drive.google.com/uc?id=1hfVAbs5nkXcUpG6FCAafid7F7ZsqRRkk\n",
        "#!curl -L --output pretrained_models/stylegan2-afhqcat-512x512.pkl 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/research/stylegan2/1/files?redirect=true&path=stylegan2-afhqcat-512x512.pkl'\n",
        "!curl -L --output pretrained_models/stylegan2-ffhq-512x512.pkl 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/research/stylegan2/1/files?redirect=true&path=stylegan2-ffhq-512x512.pkl'\n",
        "!curl -L --output pretrained_models/ViT-B-32.pt 'https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt'\n",
        "!curl -L --output pretrained_models/model_base_capfilt_large.pth https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\n",
        "#!curl -L --output pretrained_models/model_base_caption_capfilt_large.pth https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth\n",
        "\n",
        "# Download dataset:\n",
        "!gdown -O dataset.zip https://drive.google.com/uc?id=13-4a7KROKa5onLd1N4CfM36WH531rre0\n",
        "!unzip -q dataset.zip\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "Qtfg2bAL1XNb",
        "outputId": "3a8c78c4-b1a1-4648-e7c7-8a51005e730f"
      },
      "outputs": [],
      "source": [
        "# Stylegan2-ADA-PyTorch Implementation for Stylegan3 https://github.com/NVlabs/stylegan3\n",
        "\n",
        "import sys\n",
        "sys.path.append('stylegan3') # a folder stylegan3 is in the same directory as this notebook\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "import PIL.Image\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "with open('pretrained_models/stylegan2-ffhq-512x512.pkl', 'rb') as f:\n",
        "    G = pickle.load(f)['G_ema'].to(device)  # torch.nn.Module\n",
        "    \n",
        "def generate_image_stylegan2(name):\n",
        "    \n",
        "    z = torch.randn([1, G.z_dim]).to(device)\n",
        "    w = G.mapping(z, None) # styleGAN output\n",
        "    # w: 1, 16, 512\n",
        "    \n",
        "    img = G.synthesis.forward(w)\n",
        "    #z = clipout.to(device)                       # THIS IS OUR AIM #TODO # Optimize this tensor\n",
        "    #c = None                                # class labels (not used in this example)\n",
        "    #img = G(z, c)                           # NCHW, float32, dynamic range [-1, +1], no truncation\n",
        "\n",
        "    img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "    outdir = 'dataset'\n",
        "    PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/images/{name:05d}.png') # save image\n",
        "    \n",
        "    torch.save(w, f'{outdir}/tensors/{name:05d}.pt')\n",
        "    # w = torch.load('tensor.pt') # load tensor\n",
        "    #img = PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').resize((256, 256))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stylegan2 Generate Test\n",
        "'''\n",
        "for i in range(20):\n",
        "    generate_image_stylegan2(i)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "4TdxYyqx1XNe",
        "outputId": "03d9a418-0e6f-4bf7-9736-aa7ab817d8e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reshape position embedding from 196 to 1024\n",
            "load checkpoint from pretrained_models/model_base_capfilt_large.pth\n"
          ]
        }
      ],
      "source": [
        "# BLIP - Bootstrapping Language-Image Pretraining https://github.com/salesforce/BLIP\n",
        "\n",
        "# Requirements:\n",
        "# !conda install transformers=4.19.4 timm=0.9.16 fairscale=0.4.13\n",
        "\n",
        "# Download pretrained model to pwd/pretrained_models directory from:\n",
        "# !curl -LO https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\n",
        "\n",
        "import sys\n",
        "sys.path.append('BLIP') # a folder BLIP is in the same directory as this notebook\n",
        "\n",
        "from BLIP.models.blip import blip_decoder # to use class in: pwd/BLIP/models/blip.py\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "image_size = 512\n",
        "\n",
        "model_url = 'pretrained_models/model_base_capfilt_large.pth'\n",
        "#model_url = 'pretrained_models/model_base_caption_capfilt_large.pth' # Worse\n",
        "#model_url = 'pretrained_models/model_base_retrieval_coco.pth' # Assertion ERROR\n",
        "#model_url = 'pretrained_models/model_large_retrieval_coco.pth' # Assertion ERROR\n",
        "#model_url = 'pretrained_models/model_base_retrieval_flickr.pth' # Assertion ERROR\n",
        "#model_url = 'pretrained_models/model_large_caption.pth' # Assertion ERROR\n",
        "\n",
        "med_config_path = os.getcwd() + '/BLIP/configs/med_config.json'\n",
        "model = blip_decoder(pretrained=model_url, image_size=image_size, vit='base', med_config = med_config_path)\n",
        "model = model.to(device)\n",
        "    \n",
        "def get_caption_BLIP(file_num, txt_file_name):\n",
        "    filename = 'dataset/images/'+file_num+'.png'\n",
        "    raw_image = Image.open(filename).convert('RGB')\n",
        "    #w,h = raw_image.size\n",
        "    #display(raw_image.resize((w//5,h//5)))\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ])\n",
        "    image = transform(raw_image).unsqueeze(0).to(device)\n",
        "    #model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # beam search\n",
        "        caption = model.generate(image, sample=False, num_beams=3, max_length=20, min_length=10)\n",
        "        # nucleus sampling\n",
        "        # caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5)\n",
        "        print(str(file_num)+'.png caption: '+caption[0])\n",
        "        \n",
        "        with open(txt_file_name, 'a', encoding='utf-8') as txtfile:\n",
        "            txtfile.write(str(file_num) + ',' + caption[0] + '\\n')\n",
        "\n",
        "    blipout = caption[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image0001.png caption: a woman with a tooth in her mouth\n",
            "image0002.png caption: a woman with long brown hair\n",
            "image0003.png caption: a man and a woman smiling\n",
            "image0004.png caption: a little boy with glasses on\n",
            "image0005.png caption: a woman with a red flower in her hair\n",
            "image0006.png caption: a woman with a hat on\n",
            "image0007.png caption: a woman with long blonde hair\n",
            "image0008.png caption: a man in a suit and tie\n",
            "image0009.png caption: a man in a blue shirt\n",
            "image0010.png caption: a woman with long blonde hair\n",
            "image0011.png caption: a young woman with blonde hair\n",
            "image0012.png caption: a man with a beard and a blue shirt\n",
            "image0013.png caption: a woman wearing a blue hat\n",
            "image0014.png caption: a woman wearing sunglasses and smiling\n",
            "image0015.png caption: a smiling woman with glasses on\n",
            "image0016.png caption: a woman with a smile on her face\n",
            "image0017.png caption: a man with a beard and glasses\n",
            "image0018.png caption: a man with a toothy look on his face\n",
            "image0019.png caption: a man with glasses and a tie\n",
            "TXT file 'image_captions.txt' has been created successfully with image file names and their captions.\n"
          ]
        }
      ],
      "source": [
        "# BLIP Caption Save Test\n",
        "'''\n",
        "txt_name = 'image_captions.txt'\n",
        "for i in range(1,20):\n",
        "    i = str(i).zfill(5)\n",
        "    get_caption_BLIP(i,txt_name)\n",
        "print(\"TXT file '{}' has been created successfully with image file names and their captions.\".format(txt_name))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "02001.png caption: a man and a woman smiling\n",
            "02002.png caption: a little girl with a flower in her hair\n",
            "02003.png caption: a man with glasses and a tie\n",
            "02004.png caption: a man smiling with a tooth in his mouth\n",
            "02005.png caption: a man with glasses on his face\n",
            "02006.png caption: a little girl with brown hair\n",
            "02007.png caption: a bald man with a beard\n",
            "02008.png caption: a woman wearing a hat and sunglasses\n",
            "02009.png caption: a man sticking his tongue out\n",
            "02010.png caption: a little girl with long hair\n",
            "02011.png caption: a woman with a smile on her face\n",
            "02012.png caption: a man in a suit and tie\n",
            "02013.png caption: a little girl with a flower in her hair\n",
            "02014.png caption: a man with a tooth on his face\n",
            "02015.png caption: a little girl with blonde hair\n",
            "02016.png caption: a man smiling for the camera\n",
            "02017.png caption: a woman with a cell phone\n",
            "02018.png caption: a man with glasses on his face\n",
            "02019.png caption: a woman holding a bird in her hand\n",
            "02020.png caption: a woman with glasses and a microphone\n",
            "02021.png caption: a woman with a hat on\n",
            "02022.png caption: a man with a hat talking on a cell\n",
            "02023.png caption: a woman with glasses on her face\n",
            "02024.png caption: an older woman with glasses on\n",
            "02025.png caption: a woman and a man smiling\n",
            "02026.png caption: a woman smiling for the camera\n",
            "02027.png caption: a man with a cell in his hand\n",
            "02028.png caption: a young boy with a big smile\n",
            "02029.png caption: a woman with glasses on her face\n",
            "02030.png caption: a woman with long blonde hair\n",
            "02031.png caption: a woman with a tooth on her face\n",
            "02032.png caption: a man with a beard and sunglasses\n",
            "02033.png caption: a woman with glasses on her face\n",
            "02034.png caption: a man smiling for the camera\n",
            "02035.png caption: a woman with long blonde hair\n",
            "02036.png caption: a young girl with a tooth in her mouth\n",
            "02037.png caption: a woman with glasses on her face\n",
            "02038.png caption: a woman with a flower in her hair\n",
            "02039.png caption: a little girl with a tooth on her face\n",
            "02040.png caption: a woman with a tooth in her mouth\n",
            "02041.png caption: a man with a beard and a tie\n",
            "02042.png caption: a little girl smiling for the camera\n",
            "02043.png caption: an older man with glasses on\n",
            "02044.png caption: a man with glasses and a hat\n",
            "02045.png caption: a man with a hat on\n",
            "02046.png caption: a little girl with a tooth\n",
            "02047.png caption: a man in a suit and tie\n",
            "02048.png caption: a man with glasses on his face\n",
            "TXT file 'dataset/image_captions.txt' has been created successfully with image file names and their captions.\n"
          ]
        }
      ],
      "source": [
        "# COMBINED\n",
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "formatted_date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "txt_name = 'dataset/image_captions.txt' # first line should be: name,caption\n",
        "import os\n",
        "file_list = os.listdir('dataset/images/')\n",
        "file_list.sort()\n",
        "try:\n",
        "    last = int(file_list[-1][0:5])+1\n",
        "except:\n",
        "    last = 1\n",
        "    \n",
        "number_of_photos = 48\n",
        "\n",
        "for i in range(last,last+number_of_photos):\n",
        "    generate_image_stylegan2(i)\n",
        "    i = str(i).zfill(5)\n",
        "    get_caption_BLIP(i,txt_name)\n",
        "    \n",
        "print(\"TXT file '{}' has been created successfully with image file names and their captions.\".format(txt_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean extra commas from the captions\n",
        "\n",
        "input_file_path = 'dataset/image_captions.txt'\n",
        "output_file_path = 'dataset/image_captions_cleaned.txt'\n",
        "\n",
        "with open(input_file_path, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
        "    for line in input_file:\n",
        "        parts = line.strip().split(',', 1)  # Split the line into two parts at the first comma\n",
        "        if len(parts) > 1:\n",
        "            first_part = parts[0]\n",
        "            second_part = parts[1].replace(',', '')  # Remove any additional commas from the second part\n",
        "            output_file.write(first_part + ',' + second_part + '\\n')  # Write the cleaned line to the output file\n",
        "        else:\n",
        "            output_file.write(line)  # If there's no comma or only one part, write the line as is\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001</td>\n",
              "      <td>a man with a black jacket</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00002</td>\n",
              "      <td>a young child with a very look on his face</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00003</td>\n",
              "      <td>a woman with a big smile on her face</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00004</td>\n",
              "      <td>a man in a white shirt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00005</td>\n",
              "      <td>a man with a bandana on his head</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    name                                     caption\n",
              "0  00001                   a man with a black jacket\n",
              "1  00002  a young child with a very look on his face\n",
              "2  00003        a woman with a big smile on her face\n",
              "3  00004                      a man in a white shirt\n",
              "4  00005            a man with a bandana on his head"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read dataset test\n",
        "import os\n",
        "imagess = os.listdir('dataset')\n",
        "# import txt file as csv file with pandas with comma separator\n",
        "import pandas as pd\n",
        "dtype_dict = {'name': str, 'caption': str}\n",
        "df = pd.read_csv('dataset/image_captions_cleaned.txt', sep=\",\", header=0, dtype=dtype_dict)\n",
        "df.head()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
